{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Cuda is Availabe\n",
      "Using cuda:7 device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import subprocess\n",
    "import platform\n",
    "import re\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda is Availabe\")\n",
    "else:\n",
    "    print(\"Cuda Can't be found\")\n",
    "\n",
    "# Just the device selection options\n",
    "device = torch.device(\n",
    "    \"cuda:7\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "# device = torch.device(\"cpu\"\n",
    "# )\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1, padding=0)\n",
    "        self.act3 = nn.Tanh()\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1*1*120, 84)\n",
    "        self.act4 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input 1x28x28, output 6x28x28\n",
    "        x = self.act1(self.conv1(x))\n",
    "        # input 6x28x28, output 6x14x14\n",
    "        x = self.pool1(x)\n",
    "        # input 6x14x14, output 16x10x10\n",
    "        x = self.act2(self.conv2(x))\n",
    "        # input 16x10x10, output 16x5x5\n",
    "        x = self.pool2(x)\n",
    "        # input 16x5x5, output 120x1x1\n",
    "        x = self.act3(self.conv3(x))\n",
    "        # input 120x1x1, output 84\n",
    "        x = self.act4(self.fc1(self.flat(x)))\n",
    "        # input 84, output 10\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0,), (128,)),\n",
    "])\n",
    "train = torchvision.datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "test = torchvision.datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train, shuffle=True, batch_size=100)\n",
    "testloader = torch.utils.data.DataLoader(test, shuffle=True, batch_size=100)\n",
    "\n",
    "import os\n",
    "\n",
    "optimizer = None\n",
    "loss_fn = None\n",
    "\n",
    "def load_model():\n",
    "    global model, optimizer, loss_fn\n",
    "    print (os.path.curdir)\n",
    "    model = LeNet5().to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    MODEL_FILENAME='/data/home/snigdhas/SysMLProject/PruningTest-dev/model.sav'\n",
    "    if (os.path.isfile(MODEL_FILENAME) != True):\n",
    "        __train__(model, 10)\n",
    "        torch.save(model.state_dict(),MODEL_FILENAME)\n",
    "        print (f'Model saved to {MODEL_FILENAME}')\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(MODEL_FILENAME,map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "def __test__ (model__)-> float:\n",
    "    model__.eval()\n",
    "    acc = 0\n",
    "    count = 0\n",
    "    for X_batch, y_batch in testloader:\n",
    "        X_batch = X_batch.to(device); y_batch = y_batch.to(device)\n",
    "        y_pred = model__(X_batch)\n",
    "        acc += (torch.argmax(y_pred, 1) == y_batch).float().sum()\n",
    "        count += len(y_batch)\n",
    "    acc = acc / count\n",
    "    acc__ = acc.cpu()\n",
    "    return acc__.numpy()*100\n",
    "    \n",
    "\n",
    "def __train__ (model__, n_epochs=10):\n",
    "    for epoch in range(n_epochs):\n",
    "        model__.train()\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            X_batch = X_batch.to(device); y_batch = y_batch.to(device)\n",
    "            y_pred = model__(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Epoch %d: model accuracy %.2f%%\" % (epoch, __test__(model__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Static Info\n",
    "def get_chipset_info():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"lscpu\"]).decode(\"utf-8\")\n",
    "        for line in output.splitlines():\n",
    "            if \"Model name:\" in line:\n",
    "                chipset_name = line.split(\":\", 1)[1].strip()\n",
    "                return chipset_name\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    \n",
    "\n",
    "def get_arch():\n",
    "    # Get system platform information\n",
    "    return platform.machine()\n",
    "\n",
    "def get_cache_sizes():\n",
    "    cache_info = {}\n",
    "\n",
    "    # Get L1 cache size\n",
    "    output = subprocess.check_output([\"lscpu\"]).decode(\"utf-8\")\n",
    "    for line in output.splitlines():\n",
    "        if \"L1d cache:\" in line:\n",
    "            cache_info[\"L1_Size\"] = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "    # Get L2 cache size\n",
    "    output = subprocess.check_output([\"lscpu\"]).decode(\"utf-8\")\n",
    "    for line in output.splitlines():\n",
    "        if \"L2 cache:\" in line:\n",
    "            cache_info[\"L2_Size\"] = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "    # Return L1 and L2 cache sizes\n",
    "    return cache_info.get(\"L1_Size\"), cache_info.get(\"L2_Size\")\n",
    "\n",
    "def get_ddr_info():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"sudo\", \"dmidecode\", \"--type\", \"memory\"]).decode(\"utf-8\")\n",
    "        \n",
    "        ddr_name = None\n",
    "        ddr_brand = None\n",
    "        ddr_speed = None\n",
    "        ddr_info = []\n",
    "\n",
    "        for line in output.splitlines():\n",
    "            if \"Memory Device\" in line:\n",
    "                ddr_name = None\n",
    "                ddr_brand = None\n",
    "                ddr_speed = None\n",
    "            elif \"Manufacturer:\" in line:\n",
    "                ddr_brand = line.split(\":\", 1)[1].strip()\n",
    "            elif \"Type:\" in line:\n",
    "                ddr_name = line.split(\":\", 1)[1].strip()\n",
    "            elif \"Speed:\" in line:\n",
    "                ddr_speed = line.split(\":\", 1)[1].strip()\n",
    "            if ddr_name and ddr_brand and ddr_speed:\n",
    "                ddr_info.append({\"DDR_Name\": ddr_name, \"DDR_Brand\": ddr_brand, \"DDR_Speed\": ddr_speed})\n",
    "                ddr_name = None\n",
    "                ddr_brand = None\n",
    "                ddr_speed = None\n",
    "\n",
    "        if ddr_info:\n",
    "            return ddr_info\n",
    "        else:\n",
    "            return \"DDR information not found\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    \n",
    "#Dynamic Info\n",
    "def get_cpu_frequencies():\n",
    "    try:\n",
    "        with open('/proc/cpuinfo', 'r') as f:\n",
    "            cpuinfo = f.read()\n",
    "\n",
    "        cpu_frequencies = {}\n",
    "        current_core = None\n",
    "\n",
    "        for line in cpuinfo.splitlines():\n",
    "            if line.startswith('processor'):\n",
    "                current_core = line.split(':')[1].strip()\n",
    "            elif line.startswith('cpu MHz'):\n",
    "                if current_core:\n",
    "                    freq = line.split(':')[1].strip()\n",
    "                    cpu_frequencies[current_core] = freq\n",
    "\n",
    "        return cpu_frequencies\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def get_gpu_load_and_memory_used():\n",
    "    try:\n",
    "        output = subprocess.check_output([\"gpustat\"]).decode(\"utf-8\")\n",
    "        \n",
    "        gpu_info = []\n",
    "\n",
    "        for line in output.splitlines():\n",
    "            if \"|\" in line:\n",
    "                parts = line.strip().split(\"|\")\n",
    "                gpu_name = parts[0].strip()\n",
    "                gpu_load = parts[1].strip().split(\",\")[1].strip().split(\" \")[0]\n",
    "                gpu_memory_used = parts[2].strip().split(\"/\")[0].strip()\n",
    "                gpu_memory_total = parts[2].strip().split(\"/\")[1].strip().split(\" \")[0]\n",
    "                gpu_info.append({\"GPU_Load\": gpu_load, \"GPU_Memory_Used\": gpu_memory_used, \"GPU_Total_Memory\": gpu_memory_total})\n",
    "        \n",
    "        return gpu_info\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def read_psi_info(subsystem):\n",
    "    psi_file = f\"/proc/pressure/{subsystem}\"\n",
    "    if not os.path.exists(psi_file):\n",
    "        print(f\"Error: {psi_file} does not exist\")\n",
    "        return None\n",
    "\n",
    "    with open(psi_file, 'r') as file:\n",
    "        line = file.read().strip()\n",
    "        total_match = re.search(r'total=(\\d+)', line)\n",
    "        if total_match:\n",
    "            total_value = float(total_match.group(1))\n",
    "        else:\n",
    "            total_value = None\n",
    "        \n",
    "        avg10_match = re.search(r'avg10=(\\d+\\.\\d+)', line)\n",
    "        avg60_match = re.search(r'avg60=(\\d+\\.\\d+)', line)\n",
    "        avg300_match = re.search(r'avg300=(\\d+\\.\\d+)', line)\n",
    "\n",
    "        psi_info = {\n",
    "            \"total\": total_value,\n",
    "            \"avg10\": float(avg10_match.group(1)) if avg10_match else None,\n",
    "            \"avg60\": float(avg60_match.group(1)) if avg60_match else None,\n",
    "            \"avg300\": float(avg300_match.group(1)) if avg300_match else None\n",
    "        }\n",
    "        \n",
    "        return psi_info\n",
    "    \n",
    "def get_cache_info(model_name):\n",
    "    try:\n",
    "        cache_info = {}\n",
    "        cache_info[\"L1_Miss_Rate\"] = \"N/A\"\n",
    "        cache_info[\"L2_Miss_Rate\"] = \"N/A\"\n",
    "\n",
    "        if \"Intel\" in model_name:\n",
    "            perf_command = [\"timeout\", \"5s\", \"perf\", \"stat\",\"-I\", \"1000\", \"-e\", \"L1-dcache-load-misses,L1-dcache-loads,L1-dcache-stores,L1-dcache-store-misses,l2_rqsts.all_demand_miss,l2_rqsts.all_demand_references\"]\n",
    "        elif \"AMD\" in model_name:\n",
    "            perf_command = [\"timeout\", \"5s\", \"perf\", \"stat\",\"-I\", \"1000\", \"-e\", \"L1-dcache-load-misses,L1-dcache-loads,L1-dcache-stores,L1-dcache-store-misses,\\\n",
    "                            l2_request_g1.all_no_prefetch,l2_pf_hit_l2,l2_pf_miss_l2_hit_l3,l2_pf_miss_l2_l3\\\n",
    "                            l2_cache_req_stat.ic_dc_miss_in_l2 ,l2_pf_miss_l2_hit_l3,l2_pf_miss_l2_l3\"]\n",
    "        else:\n",
    "            perf_command=[]\n",
    "\n",
    "        pipe = subprocess.Popen(perf_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        pipe.terminate()\n",
    "        output, stderr = pipe.communicate()\n",
    "        if \"Intel\" in model_name:\n",
    "            for line in output.splitlines():\n",
    "                if \"L1-dcache-load-misses\" in line:\n",
    "                    L1_dcache_load_misses = int(line.split(\",\")[1].strip())\n",
    "                elif \"L1-dcache-loads\" in line:\n",
    "                    L1_dcache_loads = int(line.split(\",\")[1].strip())\n",
    "                elif \"L1-dcache-store-misses\" in line:\n",
    "                    L1_dcache_store_misses = int(line.split(\",\")[1].strip())\n",
    "                elif \"L1-dcache-store\" in line:\n",
    "                    L1_dcache_stores = int(line.split(\",\")[1].strip())\n",
    "                elif \"l2_rqsts.all_demand_miss\" in line:\n",
    "                    L2_all_demand_misses = int(line.split(\",\")[1].strip())\n",
    "                elif \"l2_rqsts.all_demand_references\" in line:\n",
    "                    L2_all_demand_references = int(line.split(\",\")[1].strip())\n",
    "            \n",
    "        elif \"AMD\" in model_name:\n",
    "            for line in output.splitlines():\n",
    "                if \"L1-dcache-load-misses\" in line:\n",
    "                    L1_dcache_load_misses = int(line.split(\",\")[1].strip())\n",
    "                elif \"L1-dcache-loads\" in line:\n",
    "                    L1_dcache_loads = int(line.split(\",\")[1].strip())\n",
    "                elif \"L1-dcache-store-misses\" in line:\n",
    "                    L1_dcache_store_misses = int(line.split(\",\")[1].strip())\n",
    "                elif \"L1-dcache-store\" in line:\n",
    "                    L1_dcache_stores = int(line.split(\",\")[1].strip())\n",
    "                elif \"l2_request_g1.all_no_prefetch\" in line:\n",
    "                    L2_all_demand_references = int(line.split(\",\")[1].strip())\n",
    "                elif \"l2_pf_hit_l2\" in line:\n",
    "                    L2_all_demand_references += int(line.split(\",\")[1].strip())\n",
    "                elif \"l2_pf_miss_l2_hit_l3\" in line:\n",
    "                    L2_all_demand_references += int(line.split(\",\")[1].strip())\n",
    "                elif \"l2_pf_miss_l2_l3\" in line:\n",
    "                    L2_all_demand_references += int(line.split(\",\")[1].strip())\n",
    "                elif \"l2_cache_req_stat.ic_dc_miss_in_l2\" in line:\n",
    "                    L2_all_demand_misses += int(line.split(\",\")[1].strip())\n",
    "                elif \"l2_pf_miss_l2_hit_l3\" in line:\n",
    "                    L2_all_demand_misses += int(line.split(\",\")[1].strip())\n",
    "                elif \"l2_pf_miss_l2_l3\" in line:\n",
    "                    L2_all_demand_misses += int(line.split(\",\")[1].strip())\n",
    "            \n",
    "        if L1_dcache_load_misses and L1_dcache_loads:\n",
    "            cache_info[\"L1_Miss_Rate\"] = L1_dcache_load_misses / L1_dcache_loads\n",
    "        if L1_dcache_store_misses and L1_dcache_stores:\n",
    "            cache_info[\"L1_Miss_Rate\"] += L1_dcache_store_misses / L1_dcache_stores\n",
    "        if L2_all_demand_misses and L2_all_demand_references:\n",
    "            cache_info[\"L2_Miss_Rate\"] = L2_all_demand_misses / L2_all_demand_references \n",
    "\n",
    "        return cache_info\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime, time,threading, psutil\n",
    "\n",
    "class SystemStatsGatherer:\n",
    "    def __init__(self,  interval=2):\n",
    "        self.temp_dict = {}\n",
    "        self.not_firstime = False\n",
    "        self.diskio_fieldname = ['read_count', 'write_count','read_bytes', 'write_bytes', 'read_time', 'write_time', 'read_merged_count', 'write_merged_count', 'busy_time']\n",
    "        self.diskio_fieldvals = [0.0 , 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0 , 0.0, 0.0]\n",
    "        self.swap_mem_fieldname = ['total', 'used', 'free', 'percent', 'sin', 'sout']\n",
    "        self.swap_mem_fieldvals = [0.0 , 0.0, 0.0, 0.0 , 0.0, 0.0]\n",
    "        self.virtual_mem_fieldname = ['total', 'available','percent', 'used', 'free', 'active', 'inactive', 'buffers', 'cached', 'shared', 'slab']\n",
    "        self.virtual_mem_fieldvals = [0.0 , 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0]\n",
    "        self.tempsensor_fieldname = ['current','high', 'critical']\n",
    "        self.tempsensor_fieldvals = [0.0 , 0.0, 0.0]\n",
    "        self.samplerThread = threading.Thread(target=self.samplerThread)\n",
    "        self.samplingInterval = interval\n",
    "        self.stopThread = False\n",
    "\n",
    "\n",
    "    def SampleTemp(self):\n",
    "\n",
    "        tempsensor_name = \"\"\n",
    "        res_temp = psutil.sensors_temperatures()\n",
    "        res_diskio = psutil.disk_io_counters(perdisk=False, nowrap=True)\n",
    "        res_swapmem = psutil.swap_memory()\n",
    "        res_virtmem = psutil.virtual_memory()\n",
    "        res_cpufreq = get_cpu_frequencies()\n",
    "        res_psicpu = read_psi_info(\"cpu\")\n",
    "        res_psimem = read_psi_info(\"memory\")\n",
    "        res_psicio = read_psi_info(\"io\")\n",
    "\n",
    "        ctr = 0\n",
    "        global not_firstime\n",
    "        ts = datetime.datetime.now()\n",
    "        if (self.not_firstime == False):\n",
    "            self.temp_dict['time'] = [str(ts)]\n",
    "        else:\n",
    "            self.temp_dict['time'] += [str(ts)]\n",
    "\n",
    "        ##get CPU frequencies\n",
    "        if (self.not_firstime == False):\n",
    "            for key, value in res_cpufreq.items():\n",
    "                self.temp_dict[f'cpu_{key}'] = float(value)\n",
    "        else:\n",
    "            for key, value in res_cpufreq.items():\n",
    "                self.temp_dict[f'cpu_{key}'] += float(value)\n",
    "\n",
    "         ##get CPU PSI\n",
    "        if (self.not_firstime == False):\n",
    "            for key, value in res_psicpu.items():\n",
    "                self.temp_dict[f'cpu_psi_{key}'] = value\n",
    "        else:\n",
    "            for key, value in res_psicpu.items():\n",
    "                self.temp_dict[f'cpu_psi_{key}'] += value\n",
    "\n",
    "         ##get Memory PSI\n",
    "        if (self.not_firstime == False):\n",
    "            for key, value in res_psimem.items():\n",
    "                self.temp_dict[f'mem_psi_{key}'] = value\n",
    "        else:\n",
    "            for key, value in res_psimem.items():\n",
    "                self.temp_dict[f'mem_psi_{key}'] += value\n",
    "\n",
    "         ##get IO PSI\n",
    "        if (self.not_firstime == False):\n",
    "            for key, value in res_psicio.items():\n",
    "                self.temp_dict[f'io_psi_{key}'] = value\n",
    "        else:\n",
    "            for key, value in res_psicio.items():\n",
    "                self.temp_dict[f'io_psi_{key}'] += value\n",
    "                \n",
    "        ## get CPU Load averages\n",
    "        loadavgs = [x / psutil.cpu_count() * 100 for x in psutil.getloadavg()]\n",
    "        if (self.not_firstime == False):\n",
    "            self.temp_dict['cpuloadavg_1min'] = loadavgs[0]\n",
    "            self.temp_dict['cpuloadavg_5min'] = loadavgs[1]\n",
    "            self.temp_dict['cpuloadavg_15min'] = loadavgs[2]\n",
    "        else:\n",
    "            self.temp_dict['cpuloadavg_1min'] += loadavgs[0]\n",
    "            self.temp_dict['cpuloadavg_5min'] += loadavgs[1]\n",
    "            self.temp_dict['cpuloadavg_15min'] += loadavgs[2]\n",
    "\n",
    "\n",
    "        ## Process the Virtual Memory readings\n",
    "        ctr = 0\n",
    "        for val in res_virtmem:\n",
    "            if (self.not_firstime == False):\n",
    "                self.temp_dict['vm_'+self.virtual_mem_fieldname[ctr]] = [val]\n",
    "            else:\n",
    "                self.temp_dict['vm_'+self.virtual_mem_fieldname[ctr]] += [val]\n",
    "            ctr += 1\n",
    "        ## Process the Swap Memory readings\n",
    "        ctr = 0\n",
    "        for val in res_swapmem:\n",
    "            if (self.not_firstime == False):\n",
    "                self.temp_dict['swap_'+self.swap_mem_fieldname[ctr]] = [val]\n",
    "            else:\n",
    "                self.temp_dict['swap_'+self.swap_mem_fieldname[ctr]] += [val]\n",
    "            ctr += 1\n",
    "        ## Process the DiskIO readings\n",
    "        ctr = 0\n",
    "        for val in res_diskio:\n",
    "            if (self.not_firstime == False):\n",
    "                self.temp_dict['diskio_'+self.diskio_fieldname[ctr]] = [val]\n",
    "            else:\n",
    "                self.temp_dict['diskio_'+self.diskio_fieldname[ctr]] += [val]\n",
    "            ctr += 1\n",
    "        # Process the Temperature readings\n",
    "        for key,values in res_temp.items():\n",
    "            # print (key,' => ',values)\n",
    "            for temp_elems in values:\n",
    "                # print (temp_elems)\n",
    "                tempsensor_name = key\n",
    "                ctr = 0;title_field = True\n",
    "                for val in temp_elems:\n",
    "                    if (title_field == True):\n",
    "                        tempsensor_name +=  '-'+str(val)\n",
    "                        title_field = False\n",
    "                    else:\n",
    "                        self.tempsensor_fieldvals[ctr] = val\n",
    "                        ctr += 1\n",
    "                        \n",
    "                    # print(key,'-', '=>',tempsensor_fieldname[ctr],'-->',sensor_val)\n",
    "                # print (tempsensor_name,'=>',tempsensor_fieldvals)\n",
    "                ctr = 0\n",
    "                for elems in self.tempsensor_fieldvals:\n",
    "                    # print (tempsensor_name+'-'+tempsensor_fieldname[ctr],'=>',elems)\n",
    "                    if (self.not_firstime == False):\n",
    "                        self.temp_dict[tempsensor_name+'-'+self.tempsensor_fieldname[ctr]] = [elems]\n",
    "                    else:\n",
    "                        self.temp_dict[tempsensor_name+'-'+self.tempsensor_fieldname[ctr]] += [elems]\n",
    "                    ctr += 1\n",
    "        self.not_firstime = True\n",
    "\n",
    "    def getReadings(self):\n",
    "        return self.temp_dict\n",
    "    \n",
    "    def samplerThread(self):\n",
    "        while (self.stopThread != True):\n",
    "            self.SampleTemp()\n",
    "            time.sleep(self.samplingInterval)\n",
    "        return\n",
    "    \n",
    "    def startSampling(self):\n",
    "        self.stopThread = False\n",
    "        self.samplerThread.start()\n",
    "        return \n",
    "\n",
    "    def stopSampling(self):\n",
    "        self.stopThread = True\n",
    "        self.samplerThread.join()\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "0.0\n",
      "Profiling run started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-04-10 03:03:18 3675636:3675636 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-04-10 03:03:56 3675636:3675636 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-04-10 03:03:57 3675636:3675636 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling run completed.\n",
      "Compiling result table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3675636/970116782.py:47: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(io.StringIO(prof_result_back), sep=r\"\\s\\s+\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 188638822.4 48672000000.0 98.05499911308289\n",
      "Collecting System Stats\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vm_percent                                            67.103361\n",
       "swap_percent                                                0.0\n",
       "cpuloadavg_1min                                        4599.975\n",
       "cpuloadavg_5min                                         4789.45\n",
       "cpu_0                                                332198.007\n",
       "cpu_1                                                331461.416\n",
       "cpu_psi_total                                  25262427857751.0\n",
       "mem_psi_total                                    163201829989.0\n",
       "io_psi_total                                     212365812362.0\n",
       "ratio                                                       0.0\n",
       "memory_consumption                                  188638822.4\n",
       "acc                                                   98.054999\n",
       "prune_config                 global_unstructured-L1Unstructured\n",
       "prune_ratio                                                 0.0\n",
       "flops                                             48672000000.0\n",
       "chipset_name          Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz\n",
       "arch                                                     x86_64\n",
       "l1_size                                                 1.3 MiB\n",
       "l2_size                                                  10 MiB\n",
       "GPU_Load_1                                                    0\n",
       "GPU_Memory_Used_1                                         27386\n",
       "GPU_Total_Memory_1                                        32768\n",
       "GPU_Load_2                                                   72\n",
       "GPU_Memory_Used_2                                         30904\n",
       "GPU_Total_Memory_2                                        32768\n",
       "GPU_Load_3                                                    0\n",
       "GPU_Memory_Used_3                                           274\n",
       "GPU_Total_Memory_3                                        32768\n",
       "GPU_Load_4                                                  100\n",
       "GPU_Memory_Used_4                                         25396\n",
       "GPU_Total_Memory_4                                        32768\n",
       "GPU_Load_5                                                  100\n",
       "GPU_Memory_Used_5                                         26322\n",
       "GPU_Total_Memory_5                                        32768\n",
       "GPU_Load_6                                                    0\n",
       "GPU_Memory_Used_6                                          3538\n",
       "GPU_Total_Memory_6                                        32768\n",
       "GPU_Load_7                                                  100\n",
       "GPU_Memory_Used_7                                         24982\n",
       "GPU_Total_Memory_7                                        32768\n",
       "GPU_Load_8                                                   79\n",
       "GPU_Memory_Used_8                                         26422\n",
       "GPU_Total_Memory_8                                        32768\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated stats\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vm_percent</th>\n",
       "      <td>67.103361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swap_percent</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpuloadavg_1min</th>\n",
       "      <td>4599.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpuloadavg_5min</th>\n",
       "      <td>4789.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu_0</th>\n",
       "      <td>332198.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu_1</th>\n",
       "      <td>331461.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cpu_psi_total</th>\n",
       "      <td>25262427857751.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mem_psi_total</th>\n",
       "      <td>163201829989.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>io_psi_total</th>\n",
       "      <td>212365812362.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratio</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memory_consumption</th>\n",
       "      <td>188638822.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acc</th>\n",
       "      <td>98.054999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prune_config</th>\n",
       "      <td>global_unstructured-L1Unstructured</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prune_ratio</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flops</th>\n",
       "      <td>48672000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chipset_name</th>\n",
       "      <td>Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arch</th>\n",
       "      <td>x86_64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l1_size</th>\n",
       "      <td>1.3 MiB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l2_size</th>\n",
       "      <td>10 MiB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Load_1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Memory_Used_1</th>\n",
       "      <td>27386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Total_Memory_1</th>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Load_2</th>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Memory_Used_2</th>\n",
       "      <td>30904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Total_Memory_2</th>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Load_3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Memory_Used_3</th>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Total_Memory_3</th>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Load_4</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Memory_Used_4</th>\n",
       "      <td>25396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Total_Memory_4</th>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Load_5</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Memory_Used_5</th>\n",
       "      <td>26322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Total_Memory_5</th>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Load_6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Memory_Used_6</th>\n",
       "      <td>3538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Total_Memory_6</th>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Load_7</th>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Memory_Used_7</th>\n",
       "      <td>24982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Total_Memory_7</th>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Load_8</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Memory_Used_8</th>\n",
       "      <td>26422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPU_Total_Memory_8</th>\n",
       "      <td>32768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            0\n",
       "vm_percent                                          67.103361\n",
       "swap_percent                                              0.0\n",
       "cpuloadavg_1min                                      4599.975\n",
       "cpuloadavg_5min                                       4789.45\n",
       "cpu_0                                              332198.007\n",
       "cpu_1                                              331461.416\n",
       "cpu_psi_total                                25262427857751.0\n",
       "mem_psi_total                                  163201829989.0\n",
       "io_psi_total                                   212365812362.0\n",
       "ratio                                                     0.0\n",
       "memory_consumption                                188638822.4\n",
       "acc                                                 98.054999\n",
       "prune_config               global_unstructured-L1Unstructured\n",
       "prune_ratio                                               0.0\n",
       "flops                                           48672000000.0\n",
       "chipset_name        Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz\n",
       "arch                                                   x86_64\n",
       "l1_size                                               1.3 MiB\n",
       "l2_size                                                10 MiB\n",
       "GPU_Load_1                                                  0\n",
       "GPU_Memory_Used_1                                       27386\n",
       "GPU_Total_Memory_1                                      32768\n",
       "GPU_Load_2                                                 72\n",
       "GPU_Memory_Used_2                                       30904\n",
       "GPU_Total_Memory_2                                      32768\n",
       "GPU_Load_3                                                  0\n",
       "GPU_Memory_Used_3                                         274\n",
       "GPU_Total_Memory_3                                      32768\n",
       "GPU_Load_4                                                100\n",
       "GPU_Memory_Used_4                                       25396\n",
       "GPU_Total_Memory_4                                      32768\n",
       "GPU_Load_5                                                100\n",
       "GPU_Memory_Used_5                                       26322\n",
       "GPU_Total_Memory_5                                      32768\n",
       "GPU_Load_6                                                  0\n",
       "GPU_Memory_Used_6                                        3538\n",
       "GPU_Total_Memory_6                                      32768\n",
       "GPU_Load_7                                                100\n",
       "GPU_Memory_Used_7                                       24982\n",
       "GPU_Total_Memory_7                                      32768\n",
       "GPU_Load_8                                                 79\n",
       "GPU_Memory_Used_8                                       26422\n",
       "GPU_Total_Memory_8                                      32768"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Profiling run started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-04-10 03:10:58 3675636:3675636 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-04-10 03:11:37 3675636:3675636 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-04-10 03:11:38 3675636:3675636 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m prof\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     32\u001b[0m acc \u001b[38;5;241m=\u001b[39m __test__(model)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mprof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProfiling run completed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCompiling result table\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m prof_result \u001b[38;5;241m=\u001b[39m prof\u001b[38;5;241m.\u001b[39mkey_averages()\u001b[38;5;241m.\u001b[39mtable(sort_by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu_memory_usage\u001b[39m\u001b[38;5;124m'\u001b[39m, row_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/profiler/profiler.py:638\u001b[0m, in \u001b[0;36mprofile.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord_steps \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_rec_fn:\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_rec_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 638\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transit_action\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/profiler/profiler.py:666\u001b[0m, in \u001b[0;36mprofile._transit_action\u001b[0;34m(self, prev_action, current_action)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_list:\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m action_list:\n\u001b[0;32m--> 666\u001b[0m         \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/profiler/profiler.py:163\u001b[0m, in \u001b[0;36m_KinetoProfile.stop_trace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstop_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/profiler.py:306\u001b[0m, in \u001b[0;36mprofile.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    298\u001b[0m parsed_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_kineto_results(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkineto_results)\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_events \u001b[38;5;241m=\u001b[39m EventList(\n\u001b[1;32m    300\u001b[0m     parsed_results,\n\u001b[1;32m    301\u001b[0m     use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cuda,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m     with_flops\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_flops,\n\u001b[1;32m    305\u001b[0m )\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_events\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/profiler_util.py:41\u001b[0m, in \u001b[0;36mEventList._build_tree\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_tree\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_populate_cpu_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_dup_nodes()\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_backward_stacktraces()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/profiler_util.py:137\u001b[0m, in \u001b[0;36mEventList._populate_cpu_children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         event\u001b[38;5;241m.\u001b[39mset_cpu_parent(parent)\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m current_events\u001b[38;5;241m.\u001b[39mappend(event)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "\n",
    "# COLUMN_NAMES = ['prune-config', 'prune-ratio','vm-percent','swap-percent','cpuloadavg-1min', 'cpuloadavg-5min', 'ratio','memory_consumption','acc']\n",
    "df_final = pd.DataFrame()\n",
    "load_model()\n",
    "chipset_name = get_chipset_info()\n",
    "arch = get_arch()\n",
    "l1_size, l2_size = get_cache_sizes()\n",
    "ddr_info = get_ddr_info()\n",
    "gpu_stats = get_gpu_load_and_memory_used()\n",
    "for ratio in np.arange(0.0,1.0,0.1):    \n",
    "    statsMon = SystemStatsGatherer()\n",
    "    statsMon.startSampling()\n",
    "    ratio = round(ratio,2)\n",
    "    print (ratio)\n",
    "    parameters = ((model.conv1, \"weight\"),(model.conv2, \"weight\"),(model.fc1, \"weight\"),(model.fc2, \"weight\"),)\n",
    "    prune.global_unstructured(parameters,pruning_method=prune.L1Unstructured,amount=ratio,)\n",
    "    for items in parameters:\n",
    "        # print(items[0],'=',items[1])\n",
    "        prune.remove(items[0],items[1])\n",
    "\n",
    "    prof = torch.profiler.profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA, torch.profiler.ProfilerActivity],profile_memory=True, record_shapes=True,with_flops=True)\n",
    "    print ('Profiling run started.')\n",
    "    prof.start()\n",
    "    acc = __test__(model)\n",
    "    prof.stop()\n",
    "\n",
    "    print ('Profiling run completed.\\nCompiling result table')\n",
    "    prof_result = prof.key_averages().table(sort_by='cpu_memory_usage', row_limit=100)\n",
    "    prof_result_back = prof_result\n",
    "\n",
    "    #Trim off the header and footer of the results\n",
    "    prof_result_back = prof_result_back.rsplit(\"\\n\",3)[0]\n",
    "    prof_result__ = \"\"\n",
    "    for line in prof_result_back:\n",
    "        if \"-\" not in line.split():\n",
    "            prof_result__ += line \n",
    "    prof_result_back = prof_result__\n",
    "    # print (prof_result_back)\n",
    "    df = pd.read_csv(io.StringIO(prof_result_back), sep=r\"\\s\\s+\")\n",
    "\n",
    "    # pd.set_option('display.max_rows', 500)\n",
    "    # display(df)\n",
    "\n",
    "    memory_consumption_str = df[(df['Name']=='[memory]')]['CPU Mem'].iloc[0]\n",
    "    flops = df[(df['Name'] == 'aten::conv2d')]['Total FLOPs'].iloc[0]\n",
    "\n",
    "    memory_consumption = 0\n",
    "    if ('Gb' in memory_consumption_str):\n",
    "        memory_consumption =  float(memory_consumption_str.split()[0])*1024*1024*1024\n",
    "    elif ('Mb' in memory_consumption_str):\n",
    "        memory_consumption =  float(memory_consumption_str.split()[0])*1024*1024\n",
    "    elif ('Kb' in memory_consumption_str):\n",
    "        memory_consumption =  float(memory_consumption_str.split()[0])*1024\n",
    "    elif ('b' in memory_consumption_str):\n",
    "        memory_consumption =  float(memory_consumption_str.split()[0])\n",
    "\n",
    "    print(ratio, memory_consumption, flops, acc)\n",
    "    statsMon.stopSampling()\n",
    "    print('Collecting System Stats')\n",
    "    df_stats = pd.DataFrame.from_dict(statsMon.getReadings())\n",
    "    agg_df = df_stats[['vm_percent','swap_percent','cpuloadavg_1min', 'cpuloadavg_5min', 'cpu_0', 'cpu_1', 'cpu_psi_total', 'mem_psi_total', 'io_psi_total']]\n",
    "    agg_df__ =  agg_df.mean()\n",
    "    agg_df__['ratio'] = ratio\n",
    "    agg_df__['memory_consumption'] = memory_consumption\n",
    "    agg_df__['acc'] = acc\n",
    "    agg_df__['prune_config'] = 'global_unstructured-L1Unstructured'\n",
    "    agg_df__['prune_ratio'] = ratio\n",
    "    agg_df__['flops'] = flops\n",
    "    agg_df__['chipset_name'] = chipset_name\n",
    "    agg_df__['arch'] = arch\n",
    "    agg_df__['l1_size'] = l1_size\n",
    "    agg_df__['l2_size'] = l2_size\n",
    "\n",
    "    \n",
    "    for i, ddr in enumerate(ddr_info):\n",
    "        for key, value in ddr.items():\n",
    "            column_name = f'{key}_{i+1}'\n",
    "            agg_df__[column_name] = value\n",
    "\n",
    "    for i, gpu_info in enumerate(gpu_stats):\n",
    "        for key, value in gpu_info.items():\n",
    "            column_name = f'{key}_{i+1}'\n",
    "            agg_df__[column_name] = value\n",
    "            \n",
    "\n",
    "    display(agg_df__)\n",
    "    df_final = pd.concat([df_final,agg_df__], axis=1)\n",
    "    print('Aggregated stats')\n",
    "    display(df_final)\n",
    "    del prof_result\n",
    "del model, loss_fn, optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vm_percent</th>\n",
       "      <th>swap_percent</th>\n",
       "      <th>cpuloadavg_1min</th>\n",
       "      <th>cpuloadavg_5min</th>\n",
       "      <th>ratio</th>\n",
       "      <th>memory_consumption</th>\n",
       "      <th>acc</th>\n",
       "      <th>prune_config</th>\n",
       "      <th>prune_ratio</th>\n",
       "      <th>flops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84.859799</td>\n",
       "      <td>90.09799</td>\n",
       "      <td>7804.852295</td>\n",
       "      <td>7624.041748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3693671874.56</td>\n",
       "      <td>98.054999</td>\n",
       "      <td>global_unstructured-L1Unstructured</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48672000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84.629299</td>\n",
       "      <td>87.907643</td>\n",
       "      <td>5094.903564</td>\n",
       "      <td>5182.476807</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3693671874.56</td>\n",
       "      <td>98.046666</td>\n",
       "      <td>global_unstructured-L1Unstructured</td>\n",
       "      <td>0.1</td>\n",
       "      <td>48672000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84.62622</td>\n",
       "      <td>90.218293</td>\n",
       "      <td>5683.044434</td>\n",
       "      <td>5860.601807</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3693671874.56</td>\n",
       "      <td>97.928333</td>\n",
       "      <td>global_unstructured-L1Unstructured</td>\n",
       "      <td>0.2</td>\n",
       "      <td>48672000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>85.228947</td>\n",
       "      <td>91.807237</td>\n",
       "      <td>6339.117432</td>\n",
       "      <td>5975.616455</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3693671874.56</td>\n",
       "      <td>97.80333</td>\n",
       "      <td>global_unstructured-L1Unstructured</td>\n",
       "      <td>0.3</td>\n",
       "      <td>48672000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85.115672</td>\n",
       "      <td>91.744776</td>\n",
       "      <td>4742.370605</td>\n",
       "      <td>4894.543457</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3682934456.32</td>\n",
       "      <td>97.364998</td>\n",
       "      <td>global_unstructured-L1Unstructured</td>\n",
       "      <td>0.4</td>\n",
       "      <td>48672000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>85.251145</td>\n",
       "      <td>91.540458</td>\n",
       "      <td>5244.519043</td>\n",
       "      <td>5323.370361</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3693671874.56</td>\n",
       "      <td>96.523333</td>\n",
       "      <td>global_unstructured-L1Unstructured</td>\n",
       "      <td>0.5</td>\n",
       "      <td>48672000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>85.458065</td>\n",
       "      <td>91.355484</td>\n",
       "      <td>5362.121582</td>\n",
       "      <td>5542.895508</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3693671874.56</td>\n",
       "      <td>96.198332</td>\n",
       "      <td>global_unstructured-L1Unstructured</td>\n",
       "      <td>0.6</td>\n",
       "      <td>48672000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>85.247191</td>\n",
       "      <td>90.474157</td>\n",
       "      <td>6722.546387</td>\n",
       "      <td>6903.692627</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3693671874.56</td>\n",
       "      <td>94.749999</td>\n",
       "      <td>global_unstructured-L1Unstructured</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48672000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>85.222857</td>\n",
       "      <td>86.100714</td>\n",
       "      <td>5032.556152</td>\n",
       "      <td>5119.189453</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3693671874.56</td>\n",
       "      <td>90.276664</td>\n",
       "      <td>global_unstructured-L1Unstructured</td>\n",
       "      <td>0.8</td>\n",
       "      <td>48672000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>85.302907</td>\n",
       "      <td>89.220349</td>\n",
       "      <td>6330.102539</td>\n",
       "      <td>6350.55542</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3693671874.56</td>\n",
       "      <td>63.983333</td>\n",
       "      <td>global_unstructured-L1Unstructured</td>\n",
       "      <td>0.9</td>\n",
       "      <td>48672000000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  vm_percent swap_percent cpuloadavg_1min cpuloadavg_5min ratio  \\\n",
       "0  84.859799     90.09799     7804.852295     7624.041748   0.0   \n",
       "1  84.629299    87.907643     5094.903564     5182.476807   0.1   \n",
       "2   84.62622    90.218293     5683.044434     5860.601807   0.2   \n",
       "3  85.228947    91.807237     6339.117432     5975.616455   0.3   \n",
       "4  85.115672    91.744776     4742.370605     4894.543457   0.4   \n",
       "5  85.251145    91.540458     5244.519043     5323.370361   0.5   \n",
       "6  85.458065    91.355484     5362.121582     5542.895508   0.6   \n",
       "7  85.247191    90.474157     6722.546387     6903.692627   0.7   \n",
       "8  85.222857    86.100714     5032.556152     5119.189453   0.8   \n",
       "9  85.302907    89.220349     6330.102539      6350.55542   0.9   \n",
       "\n",
       "  memory_consumption        acc                        prune_config  \\\n",
       "0      3693671874.56  98.054999  global_unstructured-L1Unstructured   \n",
       "1      3693671874.56  98.046666  global_unstructured-L1Unstructured   \n",
       "2      3693671874.56  97.928333  global_unstructured-L1Unstructured   \n",
       "3      3693671874.56   97.80333  global_unstructured-L1Unstructured   \n",
       "4      3682934456.32  97.364998  global_unstructured-L1Unstructured   \n",
       "5      3693671874.56  96.523333  global_unstructured-L1Unstructured   \n",
       "6      3693671874.56  96.198332  global_unstructured-L1Unstructured   \n",
       "7      3693671874.56  94.749999  global_unstructured-L1Unstructured   \n",
       "8      3693671874.56  90.276664  global_unstructured-L1Unstructured   \n",
       "9      3693671874.56  63.983333  global_unstructured-L1Unstructured   \n",
       "\n",
       "  prune_ratio          flops  \n",
       "0         0.0  48672000000.0  \n",
       "1         0.1  48672000000.0  \n",
       "2         0.2  48672000000.0  \n",
       "3         0.3  48672000000.0  \n",
       "4         0.4  48672000000.0  \n",
       "5         0.5  48672000000.0  \n",
       "6         0.6  48672000000.0  \n",
       "7         0.7  48672000000.0  \n",
       "8         0.8  48672000000.0  \n",
       "9         0.9  48672000000.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DF_SAVEFILENAME = '/Users/snigdhashekhar/PruningTest-dev/LeNet5-experiment.csv'\n",
    "df_final_transpose = df_final.transpose().reset_index(drop=True)\n",
    "display(df_final_transpose)\n",
    "df_final_transpose.to_csv(DF_SAVEFILENAME, encoding='utf-8', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
